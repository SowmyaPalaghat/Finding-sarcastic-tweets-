{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"sarcastictweet.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"wCUM13VeQEF1","colab_type":"code","colab":{}},"source":["# Common imports\n","import pandas as pd\n","from IPython.display import Markdown, display, clear_output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"O9ayM3CCROFy","colab_type":"code","colab":{}},"source":["import _pickle as cPickle\n","from pathlib import Path\n","\n","def dumpPickle(fileName, content):\n","    pickleFile = open(fileName, 'wb')\n","    cPickle.dump(content, pickleFile, -1)\n","    pickleFile.close()\n","\n","def loadPickle(fileName):    \n","    file = open(fileName, 'rb')\n","    content = cPickle.load(file)\n","    file.close()\n","    \n","    return content\n","    \n","def pickleExists(fileName):\n","    file = Path(fileName)\n","    \n","    if file.is_file():\n","        return True\n","    \n","    return False"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fPwqwSayUT_u","colab_type":"text"},"source":["extract sarcasm from the sentences# New Section"]},{"cell_type":"code","metadata":{"id":"geNPUu2SWw5r","colab_type":"code","colab":{}},"source":["#tweet\n","import tweepy\n","from tweepy import Stream\n","from tweepy import OAuthHandler\n","from tweepy.streaming import StreamListener\n","import csv\n","import time\n","\n","# you need to add yours key and tokens here...............................\n","ckey = ''\n","csecret = ''\n","atoken = ''\n","asecret = ''\n","\n","\n","class listener(StreamListener):\n","    def on_data(self, data):\n","        try:\n","            tweettext = data.split(',\"text\":\"')[1].split('\",\"source')[0]\n","            if tweettext[\n","               0:2] != 'RT':  # and '#sarcasm' not in tweettext and len(tweettext.split())>5:  #Basic additionnal filters if necessary.\n","                print(tweettext)\n","                saveFile = open('twitDB_sarcasm.csv', 'a')\n","                saveFile.write(tweettext)\n","                saveFile.write('\\n')\n","                saveFile.close()\n","            return True\n","        except BaseException as e:\n","            print('Failed ondata,', str(e))\n","            time.sleep(5)\n","\n","    def on_error(self, status):\n","        print(status)\n","\n","\n","auth = OAuthHandler(ckey, csecret)\n","auth.set_access_token(atoken, asecret)\n","\n","# NY = [74,40,-73,41]\n","twitterStream = Stream(auth, listener())\n","twitterStream.filter(track=[\"#sarcasm\"])\n","# twitterStream.filter(locations=[-122.75,36.8,-121.75,37.8,-74,40,-73,41])#locations=[-6.38,49.87,1.77,55.81])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WzfKzb0pWcT4","colab_type":"code","colab":{}},"source":["#load sentences\n","import csv, collections, os\n","import numpy as np\n","import nltk\n","\n","\n","class load_sent_word_net(object):\n","    def __init__(self):\n","        sent_scores = collections.defaultdict(list)\n","\n","        with open(os.path.join(os.path.dirname(os.path.realpath(__file__)), 'SentiWordNet_3.0.0_20130122.txt'),\n","                  'r') as csvfile:\n","\n","            reader = csv.reader(csvfile, delimiter='\\t', quotechar='\"')\n","\n","            for line in reader:\n","                if line[0].startswith('#'):\n","                    continue\n","                if len(line) == 1:\n","                    continue\n","\n","                POS, ID, PosScore, NegScore, SynsetTerms, Gloss = line\n","\n","                if len(POS) == 0 or len(ID) == 0:\n","                    continue\n","\n","                for term in SynsetTerms.split(\" \"):\n","                    term = term.split(\"#\")[0]\n","                    term = term.replace(\"-\", \" \").replace(\"_\", \" \")\n","                    key = \"%s/%s\" % (POS, term.split(\"#\")[0])\n","                    sent_scores[key].append((float(PosScore), float(NegScore)))\n","                    # this is where magic happens. You classify all words by + or -.\n","\n","        for key, value in sent_scores.items():\n","            sent_scores[key] = np.mean(value, axis=0)\n","\n","        self.sent_scores = sent_scores\n","\n","    def score_word(self, word):\n","        pos = nltk.pos_tag([word])[0][1]\n","        return self.score(word, pos)\n","\n","    def score_sentence(self, sentence):\n","        pos = nltk.pos_tag(sentence)\n","        mean_score = np.array([0.0, 0.0])\n","        for j in range(len(pos)):\n","            mean_score += self.score(pos[j][0], pos[j][1])\n","\n","        return mean_score\n","\n","    def score(self, word, pos):\n","        if pos[0:2] == 'NN':\n","            pos_type = 'n'\n","        elif pos[0:2] == 'JJ':\n","            pos_type = 'a'\n","        elif pos[0:2] == 'VB':\n","            pos_type = 'v'\n","        elif pos[0:2] == 'RB':  # adverb\n","            pos_type = 'r'\n","        else:\n","            pos_type = 0\n","\n","        if pos_type != 0:\n","            dic_loc = pos_type + '/' + word\n","            pos_neg_scores = self.sent_scores[dic_loc]  # dictionary location\n","            if len(pos_neg_scores) == 2:\n","                return pos_neg_scores\n","            else:\n","                return np.array([0.0, 0.0])\n","        else:\n","            return np.array([0.0, 0.0])\n","\n","    def posvector(self, sentence):  # position vector\n","\n","        pos_vector = nltk.pos_tag(sentence)\n","        vector = np.zeros(4)\n","\n","        for j in range(len(sentence)):\n","            pos = pos_vector[j][1]\n","            if pos[0:2] == 'NN':\n","                vector[0] += 1\n","            elif pos[0:2] == 'JJ':\n","                vector[1] += 1\n","            elif pos[0:2] == 'VB':\n","                vector[2] += 1\n","            elif pos[0:2] == 'RB':\n","                vector[3] += 1\n","\n","        return vector"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Egxa4fBrSir","colab_type":"code","outputId":"ac0b047e-72d6-485a-f7fa-cc23fd3782dd","executionInfo":{"status":"ok","timestamp":1587716411491,"user_tz":420,"elapsed":1378,"user":{"displayName":"lakshmi palaghat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3EaxnGIxybmm_SZBibVIlSSLyoSnSRC9SPo1rc64=s64","userId":"14441381853151504307"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["#topic\n","from gensim import corpora, models, similarities\n","import numpy as np\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","\n","class topic(object):\n","    def __init__(self, nbtopic=100, alpha=1, model=None, dicttp=None):\n","        self.nbtopic = nbtopic\n","        self.porter = nltk.PorterStemmer()\n","        self.alpha = alpha\n","        self.stop = stopwords.words('english') + ['.', '!', '?', '\"', '...', '\\\\', \"''\", '[', ']', '~', \"'m\", \"'s\", ';',\n","                                                  ':', '..', '$']\n","        if model != None and dicttp != None:\n","            self.lda = models.ldamodel.LdaModel.load(model)\n","            self.dictionary = corpora.Dictionary.load(dicttp)\n","\n","    def fit(self, documents):\n","        documents_mod = [replace_reg(sentence).encode() for sentence in documents]\n","        tokens = [nltk.word_tokenize(sentence).encode() for sentence in documents_mod]\n","        tokens = [[self.porter.stem(t.lower()) for t in sentence if t.lower() not in self.stop] for sentence in tokens]\n","\n","        self.dictionary = corpora.Dictionary(tokens)\n","        corpus = [self.dictionary.doc2bow(text) for text in tokens]\n","        self.lda = models.ldamodel.LdaModel(corpus, id2word=self.dictionary, num_topics=self.nbtopic, alpha=self.alpha)\n","\n","        self.lda.save('topics.tp')\n","        self.dictionary.save('topics_dict.tp')\n","\n","    def get_topic(self, topic_number):\n","        return self.lda.print_topic(topic_number)\n","\n","    def transform(self, sentence):\n","        sentence_mod = exp_replace.replace_reg(sentence).encode()\n","        tokens = nltk.word_tokenize(sentence_mod)\n","        tokens = [self.porter.stem(t.lower()) for t in tokens if t.lower() not in self.stop]\n","        corpus_sentence = self.dictionary.doc2bow(tokens)\n","\n","        return self.lda[corpus_sentence]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fx0QNzbfWzRS","colab_type":"code","colab":{}},"source":["#exp replace\n","import nltk\n","import re\n","\n","emo_repl = {\n","    # good emotions\n","    \"&lt;3\": \" good \",\n","    \":d\": \" good \",\n","    \":dd\": \" ood \",\n","    \"8)\": \" good \",\n","    \":-)\": \" good \",\n","    \":)\": \" good \",\n","    \";)\": \" good \",\n","    \"(-:\": \" good \",\n","    \"(:\": \" good \",\n","\n","    \"yay!\": \" good \",\n","    \"yay\": \" good \",\n","    \"yaay\": \" good \",\n","    \"yaaay\": \" good \",\n","    \"yaaaay\": \" good \",\n","    \"yaaaaay\": \" good \",\n","    # bad emotions\n","    \":/\": \" bad \",\n","    \":&gt;\": \" sad \",\n","    \":')\": \" sad \",\n","    \":-(\": \" bad \",\n","    \":(\": \" bad \",\n","    \":s\": \" bad \",\n","    \":-s\": \" bad \"\n","}\n","\n","emo_repl2 = {\n","    # good emotions\n","    \"&lt;3\": \" heart \",\n","    \":d\": \" smile \",\n","    \":p\": \" smile \",\n","    \":dd\": \" smile \",\n","    \"8)\": \" smile \",\n","    \":-)\": \" smile \",\n","    \":)\": \" smile \",\n","    \";)\": \" smile \",\n","    \"(-:\": \" smile \",\n","    \"(:\": \" smile \",\n","\n","    # bad emotions\n","    \":/\": \" worry \",\n","    \":&gt;\": \" angry \",\n","    \":')\": \" sad \",\n","    \":-(\": \" sad \",\n","    \":(\": \" sad \",\n","    \":s\": \" sad \",\n","    \":-s\": \" sad \"\n","}\n","\n","# general\n","re_repl = {\n","    r\"\\br\\b\": \"are\",\n","    r\"\\bu\\b\": \"you\",\n","    r\"\\bhaha\\b\": \"ha\",\n","    r\"\\bhahaha\\b\": \"ha\",\n","    r\"\\bdon't\\b\": \"do not\",\n","    r\"\\bdoesn't\\b\": \"does not\",\n","    r\"\\bdidn't\\b\": \"did not\",\n","    r\"\\bhasn't\\b\": \"has not\",\n","    r\"\\bhaven't\\b\": \"have not\",\n","    r\"\\bhadn't\\b\": \"had not\",\n","    r\"\\bwon't\\b\": \"will not\",\n","    r\"\\bwouldn't\\b\": \"would not\",\n","    r\"\\bcan't\\b\": \"can not\",\n","    r\"\\bcannot\\b\": \"can not\"\n","}\n","\n","emo_repl_order = [k for (k_len, k) in reversed(sorted([(len(k), k) for k in list(emo_repl.keys())]))]\n","emo_repl_order2 = [k for (k_len, k) in reversed(sorted([(len(k), k) for k in list(emo_repl2.keys())]))]\n","\n","\n","def replace_emo(sentence):\n","    sentence2 = sentence\n","    for k in emo_repl_order:\n","        sentence2 = sentence2.replace(k, emo_repl[k])\n","    for r, repl in re_repl.items():\n","        sentence2 = re.sub(r, repl, sentence2)\n","    return sentence2\n","    print (sentence2)\n","\n","\n","def replace_reg(sentence):\n","    sentence2 = sentence\n","    for k in emo_repl_order2:\n","        sentence2 = sentence2.replace(k, emo_repl2[k])\n","    for r, repl in re_repl.items():\n","        sentence2 = re.sub(r, repl, sentence2)\n","    return sentence2\n","    print(sentence2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XujKONUPgfu5","colab_type":"code","outputId":"cb6bd17a-e10c-44b1-b7ce-78df5c6ec374","executionInfo":{"status":"ok","timestamp":1587709048456,"user_tz":-330,"elapsed":1577,"user":{"displayName":"Lahari Penmatsa","photoUrl":"","userId":"13751974471126525587"}},"colab":{"base_uri":"https://localhost:8080/","height":74}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LEXkg83YWNTu","colab_type":"code","colab":{}},"source":["# this is how we extract features from data. We make a dictionary for all features and add features into it.\n","import nltk\n","import numpy as np\n","import string\n","from textblob import TextBlob\n","porter = nltk.PorterStemmer()\n","sentiments = load_sent_word_net()\n","\n","# all features calling.....\n","def dialogue_act_features(sentence, topic_modeler):\n","    features = {}\n","\n","    grams_feature(features, sentence)\n","    sent_feature(features, sentence)\n","    pos_feature(features, sentence)\n","    cap_feature(features, sentence)\n","    topic_feature(features, sentence, topic_modeler)\n","\n","    return features\n","\n","\n","def grams_feature(features, sentence):\n","    sentence_reg = exp_replace.replace_reg(sentence)\n","\n","    tokens = nltk.word_tokenize(sentence_reg)\n","    tokens = [porter.stem(t.lower()) for t in tokens]\n","    bigrams = nltk.bigrams(tokens)\n","    bigrams = [tup[0] + ' ' + tup[1] for tup in bigrams]\n","    grams = tokens + bigrams\n","\n","    for t in grams:\n","        features['contains(%s)' % t] = 1.0\n","\n","\n","def sent_feature(features, sentence):\n","    sentence_sentiment = exp_replace.replace_emo(sentence)\n","    tokens = nltk.word_tokenize(sentence_sentiment)\n","    tokens = [(t.lower()) for t in tokens]\n","\n","    mean_sentiment = sentiments.score_sentence(tokens)\n","    features['Positive sentiment'] = mean_sentiment[0]\n","    features['Negative sentiment'] = mean_sentiment[1]\n","    features['Sentiment'] = mean_sentiment[0] - mean_sentiment[1]\n","    try:\n","        blob = TextBlob(\n","            \"\".join([\" \" + i if not i.startswith(\"'\") and i not in string.punctuation else i for i in tokens]).strip())\n","        features['Blob sentiment'] = blob.sentiment.polarity\n","        features['Blob subjectivity'] = blob.sentiment.subjectivity\n","    except:\n","        features['Blob sentiment'] = 0.0\n","        features['Blob subjectivity'] = 0.0\n","\n","    # Split in 2\n","    if len(tokens) == 1:\n","        tokens += ['.']\n","    f = tokens[0:len(tokens) / 2]\n","    s = tokens[len(tokens) / 2:]\n","\n","    mean_sentiment_f = sentiments.score_sentence(f)\n","    features['Positive sentiment 1/2'] = mean_sentiment_f[0]\n","    features['Negative sentiment 1/2'] = mean_sentiment_f[1]\n","    features['Sentiment 1/2'] = mean_sentiment_f[0] - mean_sentiment_f[1]\n","\n","    mean_sentiment_s = sentiments.score_sentence(s)\n","    features['Positive sentiment 2/2'] = mean_sentiment_s[0]\n","    features['Negative sentiment 2/2'] = mean_sentiment_s[1]\n","    features['Sentiment 2/2'] = mean_sentiment_s[0] - mean_sentiment_s[1]\n","\n","    features['Sentiment contrast 2'] = np.abs(features['Sentiment 1/2'] - features['Sentiment 2/2'])\n","\n","    # TextBlob sentiment analysis\n","    try:\n","        blob = TextBlob(\n","            \"\".join([\" \" + i if not i.startswith(\"'\") and i not in string.punctuation else i for i in f]).strip())\n","        features['Blob sentiment 1/2'] = blob.sentiment.polarity\n","        features['Blob subjectivity 1/2'] = blob.sentiment.subjectivity\n","    except:\n","        features['Blob sentiment 1/2'] = 0.0\n","        features['Blob subjectivity 1/2'] = 0.0\n","    try:\n","        blob = TextBlob(\n","            \"\".join([\" \" + i if not i.startswith(\"'\") and i not in string.punctuation else i for i in s]).strip())\n","        features['Blob sentiment 2/2'] = blob.sentiment.polarity\n","        features['Blob subjectivity 2/2'] = blob.sentiment.subjectivity\n","    except:\n","        features['Blob sentiment 2/2'] = 0.0\n","        features['Blob subjectivity 2/2'] = 0.0\n","\n","    features['Blob Sentiment contrast 2'] = np.abs(features['Blob sentiment 1/2'] - features['Blob sentiment 2/2'])\n","\n","    # Split in 3\n","    if len(tokens) == 2:\n","        tokens += ['.']\n","    f = tokens[0:len(tokens) / 3]\n","    s = tokens[len(tokens) / 3:2 * len(tokens) / 3]\n","    t = tokens[2 * len(tokens) / 3:]\n","\n","    mean_sentiment_f = sentiments.score_sentence(f)\n","    features['Positive sentiment 1/3'] = mean_sentiment_f[0]\n","    features['Negative sentiment 1/3'] = mean_sentiment_f[1]\n","    features['Sentiment 1/3'] = mean_sentiment_f[0] - mean_sentiment_f[1]\n","\n","    mean_sentiment_s = sentiments.score_sentence(s)\n","    features['Positive sentiment 2/3'] = mean_sentiment_s[0]\n","    features['Negative sentiment 2/3'] = mean_sentiment_s[1]\n","    features['Sentiment 2/3'] = mean_sentiment_s[0] - mean_sentiment_s[1]\n","\n","    mean_sentiment_t = sentiments.score_sentence(t)\n","    features['Positive sentiment 3/3'] = mean_sentiment_t[0]\n","    features['Negative sentiment 3/3'] = mean_sentiment_t[1]\n","    features['Sentiment 3/3'] = mean_sentiment_t[0] - mean_sentiment_t[1]\n","\n","    features['Sentiment contrast 3'] = np.abs(features['Sentiment 1/3'] - features['Sentiment 3/3'])\n","\n","    try:\n","        blob = TextBlob(\n","            \"\".join([\" \" + i if not i.startswith(\"'\") and i not in string.punctuation else i for i in f]).strip())\n","        features['Blob sentiment 1/3'] = blob.sentiment.polarity\n","        features['Blob subjectivity 1/3'] = blob.sentiment.subjectivity\n","    except:\n","        features['Blob sentiment 1/3'] = 0.0\n","        features['Blob subjectivity 1/3'] = 0.0\n","    try:\n","        blob = TextBlob(\n","            \"\".join([\" \" + i if not i.startswith(\"'\") and i not in string.punctuation else i for i in s]).strip())\n","        features['Blob sentiment 2/3'] = blob.sentiment.polarity\n","        features['Blob subjectivity 2/3'] = blob.sentiment.subjectivity\n","    except:\n","        features['Blob sentiment 2/3'] = 0.0\n","        features['Blob subjectivity 2/3'] = 0.0\n","    try:\n","        blob = TextBlob(\n","            \"\".join([\" \" + i if not i.startswith(\"'\") and i not in string.punctuation else i for i in t]).strip())\n","        features['Blob sentiment 3/3'] = blob.sentiment.polarity\n","        features['Blob subjectivity 3/3'] = blob.sentiment.subjectivity\n","    except:\n","        features['Blob sentiment 3/3'] = 0.0\n","        features['Blob subjectivity 3/3'] = 0.0\n","\n","    features['Blob Sentiment contrast 3'] = np.abs(features['Blob sentiment 1/3'] - features['Blob sentiment 3/3'])\n","\n","\n","def pos_feature(features, sentence):\n","    sentence_pos = exp_replace.replace_emo(sentence)\n","    tokens = nltk.word_tokenize(sentence_pos)\n","    tokens = [(t.lower()) for t in tokens]\n","    pos_vector = sentiments.posvector(tokens)\n","    for j in range(len(pos_vector)):\n","        features['POS' + str(j + 1)] = pos_vector[j]\n","\n","\n","def cap_feature(features, sentence):\n","    counter = 0\n","    treshold = 4\n","    for j in range(len(sentence)):\n","        counter += int(sentence[j].isupper())\n","    features['Capitalization'] = int(counter >= treshold)  # true : 1, false : 0\n","\n","\n","def topic_feature(features, sentence, topic_modeler):\n","    topics = topic_modeler.transform(sentence)\n","\n","    for j in range(len(topics)):\n","        features['Topic :' + str(topics[j][0])] = topics[j][1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nOm_dQB4V9pz","colab_type":"code","colab":{}},"source":["#evaluate\n","import numpy as np\n","import pickle\n","import os\n","\n","\n","# pickle files....\n","obj1 = open(os.path.join(os.path.dirname(os.path.realpath(__file__)), 'vecdict.p'), 'r')\n","obj2 = open(os.path.join(os.path.dirname(os.path.realpath(__file__)), 'classif.p'), 'r')\n","\n","# vector of features of all the data.......\n","vec = pickle.load(obj1)\n","# SVC classifier that is already trained in traintest.py file......\n","classifier = pickle.load(obj2)\n","\n","obj1.close()\n","obj2.close()\n","\n","# this is required because of various OS have various slashes in paths...\n","topic_mod = topic(model=os.path.join(os.path.dirname(os.path.realpath(__file__)), 'topics.tp'),\n","                        dicttp=os.path.join(os.path.dirname(os.path.realpath(__file__)), 'topics_dict.tp'))\n","\n","\n","def tweetscore(sentence):\n","    features = feature_extract.dialogue_act_features(sentence, topic_mod)\n","    # classifier can only get data in numerical from so convert in vector form.\n","    features_vec = vec.transform(features)\n","    score = classifier.decision_function(features_vec)[0]\n","    # sigmoid and other manipulations........\n","    per = int(round(2.0 * (1.0 / (1.0 + np.exp(-score)) - 0.5) * 1000.0))\n","\n","    return per"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t81nXTBNVrxt","colab_type":"code","colab":{}},"source":["#sarcasm detector\n","import os\n","import flask, flask.views\n","from flask import Markup\n","from flask import jsonify\n","\n","app = flask.Flask(__name__)\n","\n","\n","class Main(flask.views.MethodView):\n","    def get(self):\n","        return flask.render_template('index.html')\n","\n","\n","\n","app.add_url_rule('/', view_func=Main.as_view('main'), methods=[\"GET\"])\n","\n","\n","\n","@app.route('/_compute')\n","def compute():\n","    sentence = flask.request.args.get('sentence')\n","    percentage = evaluate.tweetscore(str(sentence))\n","    return jsonify(result=percentage)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yAjgVMS1yeSm","colab_type":"code","colab":{}},"source":["#preprocess\n","import numpy as np\n","import csv\n","import re\n","\n","\n","def preprocessing(csv_file_object):\n","    data = []\n","    length = []\n","    remove_hashtags = re.compile(r'#\\w+\\s?')\n","    remove_friendtag = re.compile(r'@\\w+\\s?')\n","    remove_sarcasm = re.compile(re.escape('sarcasm'), re.IGNORECASE)\n","    remove_sarcastic = re.compile(re.escape('sarcastic'), re.IGNORECASE)\n","\n","    for row in csv_file_object:\n","        if len(row[0:]) == 1:\n","            temp = row[0:][0]\n","            temp = remove_hashtags.sub('', temp)\n","            if len(temp) > 0 and 'http' not in temp and temp[0] != '@' and '\\\\u' not in temp:\n","                temp = remove_friendtag.sub('', temp)\n","                temp = remove_sarcasm.sub('', temp)\n","                temp = remove_sarcastic.sub('', temp)\n","                temp = ' '.join(temp.split())  # remove useless space\n","                if len(temp.split()) > 2:\n","                    data.append(temp)\n","                    length.append(len(temp.split()))\n","    data = list(set(data))\n","    data = np.array(data)\n","\n","    return data, length\n","\n","\n","print('Extracting data')\n","\n","\n","csv_file_object_pos = csv.reader(open('twitDB_sarcasm.csv', 'rU'), delimiter='\\n')\n","pos_data, length_pos = preprocessing(csv_file_object_pos)\n","\n","\n","csv_file_object_neg = csv.reader(open('twitDB_regular.csv', 'rU'), delimiter='\\n')\n","neg_data, length_neg = preprocessing(csv_file_object_neg)\n","\n","print('Number of  sarcastic tweets :', len(pos_data))\n","print('Average length of sarcastic tweets :', np.mean(length_pos))\n","print('Number of  non-sarcastic tweets :', len(neg_data))\n","print('Average length of non-sarcastic tweets :', np.mean(length_neg))\n","\n","# .npy files...\n","np.save('posproc', pos_data)\n","np.save('negproc', neg_data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cYTNK5y1zCG7","colab_type":"code","outputId":"9f0f95ee-7f3e-449e-b255-c32849dd8842","executionInfo":{"status":"error","timestamp":1587709080204,"user_tz":-330,"elapsed":4559,"user":{"displayName":"Lahari Penmatsa","photoUrl":"","userId":"13751974471126525587"}},"colab":{"base_uri":"https://localhost:8080/","height":470}},"source":["#train and test\n","import nltk\n","import numpy as np\n","import scipy as sp\n","from sklearn.utils import shuffle\n","from sklearn.svm import LinearSVC\n","from sklearn.metrics import classification_report\n","from sklearn.feature_extraction import DictVectorizer\n","import pickle\n","import heapq\n","\n","print('Pickling out')\n","pos_data = np.load('/content/drive/My Drive/Colab Notebooks/posproc.npy')\n","print(pos_data)\n","neg_data = np.load('/content/drive/My Drive/Colab Notebooks/negproc.npy')\n","print(neg_data)\n","print('Number of  sarcastic tweets :', len(pos_data))\n","print('Number of  non-sarcastic tweets :', len(neg_data))\n","\n","print('Training topics')\n","# topic model\n","topic()\n","topic_mod = topic(nbtopic=200, alpha='symmetric')\n","topic_mod.fit(np.concatenate((pos_data, neg_data)))\n","\n","print('Feature eng')\n","# label set\n","cls_set = ['Non-Sarcastic', 'Sarcastic']\n","featuresets = []\n","\n","index = 0\n","for tweet in pos_data:\n","    if (np.mod(index, 10000) == 0):\n","        print(\"Positive tweet processed: \", index)\n","    featuresets.append((feature_extract.dialogue_act_features(tweet, topic_mod), cls_set[1]))\n","    index += 1\n","\n","index = 0\n","for tweet in neg_data:\n","    if np.mod(index, 10000) == 0:\n","        print(\"Negative tweet processed: \", index)\n","    featuresets.append((feature_extract.dialogue_act_features(tweet, topic_mod), cls_set[0]))\n","    index += 1\n","\n","featuresets = np.array(featuresets)\n","targets = (featuresets[0::, 1] == 'Sarcastic').astype(int)\n","\n","print('Dictionnary vectorizer')\n","vec = DictVectorizer()\n","# classifier can only get data in numerical from so convert in vector form.\n","featurevec = vec.fit_transform(featuresets[0::, 0])\n","\n","# Saving the dictionnary vectorizer\n","file_Name = \"vecdict.p\"\n","# wb : write binary mode\n","fileObject = open(file_Name, 'wb')\n","pickle.dump(vec, fileObject)\n","fileObject.close()\n","\n","print('Feature splitting')\n","# Shuffling\n","order = shuffle(list(range(len(featuresets))))\n","# shuffling in same order....\n","targets = targets[order]\n","featurevec = featurevec[order, 0::]\n","\n","# Splitting\n","size = int(len(featuresets) * .3)  # 30% is used for the test set\n","\n","trainvec = featurevec[size:, 0::]\n","train_targets = targets[size:]\n","testvec = featurevec[:size, 0::]\n","test_targets = targets[:size]\n","\n","print('Training')\n","\n","# Artificial weights\n","pos_p = (train_targets == 1)\n","neg_p = (train_targets == 0)\n","ratio = np.sum(neg_p.astype(float)) / np.sum(pos_p.astype(float))\n","new_trainvec = trainvec\n","new_train_targets = train_targets\n","# THE VERY CORE PART OF WHOLE PROJECT................\n","for j in range(int(ratio - 1.0)):\n","    new_trainvec = sp.sparse.vstack([new_trainvec, trainvec[pos_p, 0::]])\n","    new_train_targets = np.concatenate((new_train_targets, train_targets[pos_p]))\n","\n","classifier = LinearSVC(C=0.1, penalty='l2', dual=True)\n","classifier.fit(new_trainvec, new_train_targets)\n","\n","# Saving the classifier\n","file_Name = \"classif.p\"\n","fileObject = open(file_Name, 'wb')\n","pickle.dump(classifier, fileObject)\n","fileObject.close()\n","\n","# OUR TASK ENDED HERE. OBJECT FILES ARE SAVED. FROM NOW ON JUST PLAYING WITH DATA...........\n","print('Most important features')\n","\n","print('grams:')\n","\n","# coef_ gives weights assigned to features....\n","coeff = vec.inverse_transform(classifier.coef_[0])[0]\n","largest = heapq.nlargest(int(100 / 2.0), coeff, key=coeff.get)\n","smallest = heapq.nsmallest(int(100 / 2.0), coeff, key=coeff.get)\n","for j in range(int(100 / 2.0)):\n","    print(largest[j], coeff[largest[j]])\n","for j in range(int(100 / 2.0)):\n","    print(smallest[j], coeff[smallest[j]])\n","\n","print('sentiment:')\n","\n","print('Positive sentiment', coeff['Positive sentiment'])\n","print('Positive sentiment 1/2', coeff['Positive sentiment 1/2'])\n","print('Positive sentiment 2/2', coeff['Positive sentiment 2/2'])\n","print('Positive sentiment 1/3', coeff['Positive sentiment 1/3'])\n","print('Positive sentiment 2/3', coeff['Positive sentiment 2/3'])\n","print('Positive sentiment 3/3', coeff['Positive sentiment 3/3'])\n","print('Negative sentiment', coeff['Negative sentiment'])\n","print('Negative sentiment 1/2', coeff['Negative sentiment 1/2'])\n","print('Negative sentiment 2/2', coeff['Negative sentiment 2/2'])\n","print('Negative sentiment 1/3', coeff['Negative sentiment 1/3'])\n","print('Negative sentiment 2/3', coeff['Negative sentiment 2/3'])\n","print('Negative sentiment 3/3', coeff['Negative sentiment 3/3'])\n","\n","print('Blob sentiment', coeff['Blob sentiment'])\n","print('Blob subjectivity', coeff['Blob subjectivity'])\n","print('Blob sentiment 1/2', coeff['Blob sentiment 1/2'])\n","print('Blob sentiment 2/2', coeff['Blob sentiment 2/2'])\n","print('Blob subjectivity 1/2', coeff['Blob subjectivity 1/2'])\n","print('Blob subjectivity 2/2', coeff['Blob subjectivity 2/2'])\n","print('Blob sentiment 1/3', coeff['Blob sentiment 1/3'])\n","print('Blob sentiment 2/3', coeff['Blob sentiment 2/3'])\n","print('Blob sentiment 3/3', coeff['Blob sentiment 3/3'])\n","print('Blob subjectivity 1/3', coeff['Blob subjectivity 1/3'])\n","print('Blob subjectivity 2/3', coeff['Blob subjectivity 2/3'])\n","print('Blob subjectivity 3/3', coeff['Blob subjectivity 3/3'])\n","\n","print('topics:')\n","\n","topics_tag = []\n","topics_coeff = []\n","topics_num = []\n","for j in range(200):\n","    topics_tag.append('Topic :' + str(j))\n","    topics_coeff.append(coeff[topics_tag[j]])\n","    topics_num.append(j)\n","topics_tag = np.array(topics_tag)\n","topics_num = np.array(topics_num)\n","topics_coeff = np.array(topics_coeff)\n","\n","# Returns the indices that would sort the array.\n","topics_num = topics_num[topics_coeff.argsort()]\n","topics_tag = topics_tag[topics_coeff.argsort()]\n","topics_coeff = topics_coeff[topics_coeff.argsort()]\n","for j in range(10):\n","    print(topics_coeff[j], topic_mod.get_topic(topics_num[j]))\n","for j in range(190, 200):\n","    print(topics_coeff[j], topic_mod.get_topic(topics_num[j]))\n","\n","print('Validating')\n","\n","output = classifier.predict(testvec)\n","print(classification_report(test_targets, output, target_names=cls_set))\n","\n","# BASIC TEST\n","basic_test = [\"This is just a long sentence, to make sure that it's not how long the sentence is that matters the most\",\n","              'I just love when you make me feel like shit', 'Life is odd', 'Just got back to the US !',\n","              \"Isn'it great when your girlfriend dumps you ?\", \"I love my job !\", 'I love my son !', \"I love my life !!\"]\n","feature_basictest = []\n","for tweet in basic_test:\n","    # get feature vector for each tweet\n","    feature_basictest.append(feature_extract.dialogue_act_features(tweet, topic_mod))\n","feature_basictest = np.array(feature_basictest)\n","# classifier can only get data in numerical from so convert in vector form.\n","feature_basictestvec = vec.transform(feature_basictest)\n","\n","print(basic_test)\n","print(classifier.predict(feature_basictestvec))\n","print(classifier.decision_function(feature_basictestvec))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Pickling out\n","[b'My family is so open and loving.'\n"," b\"What who did to who? Others? Let America decide what to do? That's helpful advice!\"\n"," b'and texts dont go' ... b'I love having so many friends'\n"," b'WoW what a great night.' b'now listening to: acdc-back in black\\\\n']\n","[b'Yo $20 to gets me and takes me to mcdonalds'\n"," b'A nigga will fuck anything ugh'\n"," b\"I hate when people try to tell me about me. You don't know me.\" ...\n"," b'Anyone wanna go kayaking with me tomorrow'\n"," b'Finals MVP Green or Leonard'\n"," b'Success Is Not Final Failure Is Not Fatal: It Is The Courage To Continue That Counts.']\n","Number of  sarcastic tweets : 25273\n","Number of  non-sarcastic tweets : 117825\n","Training topics\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-6e0e149972eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training topics'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# topic model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtopic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mtopic_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbtopic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'symmetric'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mtopic_mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'topic' is not defined"]}]},{"cell_type":"code","metadata":{"id":"xcGFeaNZqvxz","colab_type":"code","colab":{}},"source":["#!/usr/bin/env python\n","# -*- coding: utf-8 -*-\n","\n","# Note: To use the 'upload' functionality of this file, you must:\n","pip install twine\n","\n","import io\n","import os\n","import sys\n","from shutil import rmtree\n","\n","from setuptools import find_packages, setup, Command\n","\n","# Package meta-data.\n","NAME = 'deeptranslit'\n","DESCRIPTION = 'Transliteration with Deep Learning'\n","URL = 'https://github.com/bedapudi6788/deeptranslit'\n","EMAIL = 'praneethbedapudi@gmail.com'\n","AUTHOR = 'BEDAPUDI PRANEETH'\n","REQUIRES_PYTHON = '>=3.6.0'\n","VERSION = '1.0.1'\n","\n","# What packages are required for this module to be executed?\n","REQUIRED = [\n","    'txt2txt'\n","]\n","\n","# What packages are optional?\n","EXTRAS = {\n","    # 'fancy feature': ['django'],\n","}\n","\n","# The rest you shouldn't have to touch too much :)\n","# ------------------------------------------------\n","# Except, perhaps the License and Trove Classifiers!\n","# If you do change the License, remember to change the Trove Classifier for that!\n","\n","\n","here = os.path.abspath(os.path.dirname('__file__'))\n","\n","# Import the README and use it as the long-description.\n","# Note: this will only work if 'README.md' is present in your MANIFEST.in file!\n","try:\n","    with io.open(os.path.join(here, 'README.md'), encoding='utf-8') as f:\n","        long_description = '\\n' + f.read()\n","except FileNotFoundError:\n","    long_description = DESCRIPTION\n","\n","# Load the package's __version__.py module as a dictionary.\n","about = {}\n","if not VERSION:\n","    with open(os.path.join(here, NAME, '__version__.py')) as f:\n","        exec(f.read(), about)\n","else:\n","    about['__version__'] = VERSION\n","\n","\n","class UploadCommand(Command):\n","    \"\"\"Support setup.py upload.\"\"\"\n","\n","    description = 'Build and publish the package.'\n","    user_options = []\n","\n","    @staticmethod\n","    def status(s):\n","        \"\"\"Prints things in bold.\"\"\"\n","        print('\\033[1m{0}\\033[0m'.format(s))\n","\n","    def initialize_options(self):\n","        pass\n","\n","    def finalize_options(self):\n","        pass\n","\n","    def run(self):\n","        try:\n","            self.status('Removing previous builds…')\n","            rmtree(os.path.join(here, 'dist'))\n","        except OSError:\n","            pass\n","\n","        self.status('Building Source and Wheel (universal) distribution…')\n","        os.system('{0} setup.py sdist bdist_wheel --universal'.format(sys.executable))\n","\n","        self.status('Uploading the package to PyPI via Twine…')\n","        os.system('twine upload dist/*')\n","\n","        self.status('Pushing git tags…')\n","        os.system('git tag v{0}'.format(about['__version__']))\n","        os.system('git push --tags')\n","        \n","        sys.exit()\n","\n","\n","# Where the magic happens:\n","setup(\n","    name=NAME,\n","    version=about['__version__'],\n","    description=DESCRIPTION,\n","    long_description=long_description,\n","    long_description_content_type='text/markdown',\n","    author=AUTHOR,\n","    author_email=EMAIL,\n","    python_requires=REQUIRES_PYTHON,\n","    url=URL,\n","    packages=find_packages(exclude=('tests',)),\n","    # If your package is a single module, use this instead of 'packages':\n","    # py_modules=['mypackage'],\n","\n","    # entry_points={\n","    #     'console_scripts': ['mycli=mymodule:cli'],\n","    # },\n","    install_requires=REQUIRED,\n","    extras_require=EXTRAS,\n","    include_package_data=True,\n","    license='GPLv3',\n","    classifiers=[\n","        # Trove classifiers\n","        # Full list: https://pypi.python.org/pypi?%3Aaction=list_classifiers\n","        'License :: OSI Approved :: GNU General Public License v3 (GPLv3)',\n","        'Programming Language :: Python',\n","        'Programming Language :: Python :: 3',\n","        'Programming Language :: Python :: 3.6',\n","        'Programming Language :: Python :: Implementation :: CPython',\n","        'Programming Language :: Python :: Implementation :: PyPy'\n","    ],\n","    #setup.py publish support.\n","    cmdclass={\n","        'upload': UploadCommand,\n","    },\n",")   "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CYuluf3Di3PQ","colab_type":"code","colab":{}},"source":["import os\n","import re\n","import pickle\n","import string\n","import pydload\n","import logging\n","import itertools\n","\n","from txt2txt import infer, build_model\n","\n","kenlm_available = True\n","\n","try:\n","    import kenlm\n","except:\n","    logging.warn('KenLm not installed. Simple scoring will be used.')\n","    kenlm_available = False\n","\n","model_links = {\n","            'hi': {\n","                    'checkpoint': 'https://github.com/bedapudi6788/DeepTranslit/releases/download/v0.5/en_hi_checkpoint',\n","                    'params': 'https://github.com/bedapudi6788/DeepTranslit/releases/download/v0.5/en_hi_params',\n","                    'words': 'https://github.com/bedapudi6788/DeepTranslit/releases/download/v0.5/hi_words',\n","                    'lm': 'https://github.com/bedapudi6788/DeepTranslit/releases/download/v0.5/hi_lm.bin'\n","                },\n","            }\n","\n","lang_code_mapping = {\n","            'hindi': 'hi',\n","        }\n","\n","class DeepTranslit():\n","    params = None\n","    model = None\n","    words = None\n","    lm = None\n","    rank = 'auto'\n","\n","    def __init__(self, lang_code, rank='auto'):\n","        \"\"\"\n","        Initialize deeptranslit\n","        Parameters:\n","        lang_code (str): Name or code of the language. (Currently supported: hindi/hi)\n","        rank (str): Mode of ranking. In default mode ('auto') kenlm will be used if available. (simple|kenlm|auto are the supported options)\n","        \"\"\"\n","\n","        if lang_code in lang_code_mapping:\n","            lang_code = lang_code_mapping[lang_code]\n","        \n","        if lang_code not in model_links:\n","            print(\"DeepTranslit doesn't support '\" + lang_code + \"' yet.\")\n","            print(\"Please raise a issue at https://github.com/bedapudi6788/deeptranslit to add this language into future checklist.\")\n","            return None\n","        \n","        # loading the model\n","        home = os.path.expanduser(\"~\")\n","        lang_path = os.path.join(home, '.DeepTranslit_' + lang_code)\n","        checkpoint_path = os.path.join(lang_path, 'checkpoint')\n","        params_path = os.path.join(lang_path, 'params')\n","        words_path = os.path.join(lang_path, 'words')\n","        lm_path = os.path.join(lang_path, 'lm')\n","        \n","        if not os.path.exists(lang_path):\n","            os.mkdir(lang_path)\n","\n","        if not os.path.exists(checkpoint_path):\n","            print('Downloading checkpoint', model_links[lang_code]['checkpoint'], 'to', checkpoint_path)\n","            pydload.dload(url=model_links[lang_code]['checkpoint'], save_to_path=checkpoint_path, max_time=None)\n","\n","        if not os.path.exists(params_path):\n","            print('Downloading model params', model_links[lang_code]['params'], 'to', params_path)\n","            pydload.dload(url=model_links[lang_code]['params'], save_to_path=params_path, max_time=None)\n","        \n","        if not os.path.exists(words_path):\n","            print('Downloading words', model_links[lang_code]['words'], 'to', words_path)\n","            pydload.dload(url=model_links[lang_code]['words'], save_to_path=words_path, max_time=None)\n","\n","        if not os.path.exists(lm_path):\n","            print('Downloading lm', model_links[lang_code]['lm'], 'to', lm_path)\n","            pydload.dload(url=model_links[lang_code]['lm'], save_to_path=lm_path, max_time=None)\n","        \n","        DeepTranslit.model, DeepTranslit.params = build_model(params_path=params_path, enc_lstm_units=64, use_gru=True, display_summary=False)\n","        DeepTranslit.model.load_weights(checkpoint_path)\n","\n","        DeepTranslit.words = pickle.load(open(words_path, 'rb'))\n","\n","        if kenlm_available and rank in {'auto', 'kenlm'}:\n","            logging.warn('Loading KenLM.')\n","            DeepTranslit.lm = kenlm.Model(lm_path)\n","            DeepTranslit.rank = rank\n","\n","    def transliterate(self, sent, top=3):\n","        \"\"\"\n","        Transliterate an input sentence while preserving punctuation at word or sentence endings.\n","        Parameters:\n","        sent (str): Sentence to be transliterated.\n","        top (int): top-n results to be returned. if 0 or None, all results will be returned.\n","        Returns:\n","        list: returns list of tuples of size 2 with first element of each tuple being the transliterated sentence and second element being the \"score\"\n","        \"\"\"\n","        rank = DeepTranslit.rank\n","        words = sent.strip().split()\n","        puncs = []\n","        for i, word in enumerate(words):\n","            words[i] = re.sub('[' + string.punctuation + ']', '', word.lower())\n","            if not words[i]:\n","                continue\n","\n","            punc = None\n","            if word[-1] in string.punctuation:\n","                punc = word[-1]\n","            \n","            puncs.append(punc)\n","\n","        words = [w for w in words if w]\n","        \n","        np_words = []\n","\n","        for i, word in enumerate(words):\n","            if [c for c in word if c not in DeepTranslit.params['input_encoding']]:\n","                np_words.append((i - len(np_words), word))\n","                words[i] = None\n","            \n","        words = [w for w in words if w]        \n","\n","        preds = infer(words, DeepTranslit.model, DeepTranslit.params)\n","\n","        for posi, np_word in np_words:\n","            preds = preds[:posi] + [[{'sequence': np_word, 'prob': 1}]] + preds[posi:]\n","\n","        resp = []\n","\n","        preds = list(itertools.product(*preds))\n","\n","        for pred in preds:\n","            words = [w['sequence'] for w in pred]\n","            for i, word in enumerate(words):\n","                if puncs[i]:\n","                    word = word + puncs[i]\n","                words[i] = word\n","\n","            probs = [w['prob'] for w in pred]\n","\n","            sent = ' '.join(words)\n","            resp.append((sent, probs))\n","        \n","        if rank == 'auto':\n","            if kenlm_available:\n","                rank = 'kenlm'\n","            else:\n","                rank = 'simple'\n","\n","        if rank == 'simple':\n","            for i, (sent, probs) in enumerate(resp):\n","                words = sent.split()\n","                score = sum([1 for word in words if word in DeepTranslit.words])\n","                resp[i] = (sent, score)\n","            \n","            resp = sorted(resp, key=lambda x: x[1], reverse=True)\n","        \n","        elif rank == 'kenlm':\n","            if not kenlm_available:\n","                logging.error(\"KenLm not available\")\n","                return resp\n","\n","            for i, (sent, probs) in enumerate(resp):\n","                score = DeepTranslit.lm.score(sent)\n","                resp[i] = (sent, score)\n","\n","            resp = sorted(resp, key=lambda x: x[1], reverse=True) \n","        \n","\n","        if top:\n","            resp = resp[:top]\n","\n","        return resp"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z94tt6I9xN4C","colab_type":"code","colab":{}},"source":["#transliterator = DeepTranslit('hindi')\n","# Currently hindi (hi) is the only supported language.\n","\n","#transliterator.transliterate('mai mbbs complete karliya')"],"execution_count":0,"outputs":[]}]}